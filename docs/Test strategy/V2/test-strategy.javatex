 packages ##
fontenc[T1]
color
amssymb
mathrsfs
amsmath
eurosym
graphicx
textcomp
listings
epigraph
longtable
setspace
background[some]
gensymb
tikz
tabularx
geometry
fancyhdr
babel[english]
##
 commands ##
js-html-css.listing
scala.listing
java.listing
eclipse.listing
##
 documentSettings ##
documentClass=report
alinea=4mm
chapterName=Part
##
 preprocessor ##

##
> title[titlePage.projet-merouTemplate] ##
date=March $9^{rd}$, 2015
ref=model-checking.test-strategy
first_author_name=Sofia \textsc{Boutahar} ~\\ David \textsc{Courtinot}
title_size=0.9
sup_strip_color=0.70,0.70,0.70
inf_strip_color=0.00,0.00,0.00
title=Test strategy and results
version=3
##
>> latex ##
\chapter* {Changelog}
\begin{center}
\begin{tabular}{|c|l|l|}
  \hline
  Version & Date & Change  \\
  \hline
  V1 & February $16^{th}$ & first version \\
  \hline
  V2 & March $1^{rd}$  & Added the configuration of the tests \\
  & March $1^{rd}$  & Minor redaction fixes \\
  & March $1^{rd}$  & Added more detailed explanations in all the parts \\
  & March $2^{nd}$  & Added CFG part \\
  & March $3^{rd}$  & Added the newest tests for the model checker \\
  \hline
  V3 & March $9^{th}$  & Updated the overview \\
  & March $9^{th}$  & Added references to the sprint backlogs (see all parts)  \\
  & March $9^{th}$  & Added references to constraints detailed by the development plan (see all parts) \\
  & March $9^{th}$ & Various minor fixes \\
  & March $9^{th}$ & Justified the testing approach for the two steps of the AST fo CFG conversion \\
  & March $9^{th}$ & Explained why the tests of the \textit{for} loop conversion to CFG are exhaustive\\
  \hline
\end{tabular}
\end{center}
##
>> latex ##
\chapter* {Overview}
##
>>> paragraph ##
The purpose of a test strategy is to clarify the major tasks and challenges of the test project.
Moreover, it helps us figuring out if there are missing requirements in the project and have a clear state of it at any point, especially in regard of our current sprint objectives.
Finally, this test strategy aims to prove our program meets the requirements of our client. To achieve it, 
we will refer to the constraints and expected functionalities described in the development plan (reference : model-checking.dev-plan).
Our project is divided into two parts : The conversion of the AST to the CFG and the model checking using CTL.
The first part can be divided into two subparts : parsing the file generated by the Clang compiler and converting the AST to the CFG.
##
>>> paragraph ##

##
>> chapter ##
Testing AST conversion to CFG
##
>>> section ##
Test strategy
##
>>>> subsection ##
Tools and configuration
##
>>>>> paragraph ##
In order to partly automate our tests and improve the readbility of the output, our test routines automatically call some external tools with the following settings :
##
>>>>> list ##
\textbf{Clang API}, version 3.5, options : \textit{-Xclang -ast-dump -std=c++11 -fsyntax-only -w};
the \textbf{dot} command, which is part of GraphViz, version 2.29
##
>>>>> paragraph ##
Finally, the code was compiled with the official Scala 2.10 compiler.
##
>>>> subsection ##
Approach
##
>>>>> paragraph ##
When possible, we tried defining automatic test routines or at least to automate all the steps to get the final output, which may have to be manually checked.
If one of our tests fails, we fix it, running the same test over and over until the output is correct.
In some cases, we consider that the parts of the source code concerned by the fix could have an impact on other functionalities. Thus, we check the fix against the other
tests (that were passed prior to the fix). Tests considered as mutually dependent are reunited in the same folder containing the test input files.
##
>>> section ##
Testing the parser
##
>>>> paragraph ##
title=Note :
The tests we are going to present concern the sprint backlog 1, as the parser should be finished during the first sprint.
They cover all the constraints related to the parsing from 1.0.1 to 1.0.4. Indeed, as the above test description illustrates it, there is no loss
of information in our parsed AST at all on the purely imperative C++ language compared to the information of the orignal AST. Constraint 1.0.5 was abandoned after a precision of the client's needs.
##
>>>> paragraph ##
As the parser simply translates the AST (produced by Clang) in an in-memory tree data-structure, we simply needed to check that
each node of the original AST was correctly converted and that the structure of our parsed AST was the same as the original one.
This cannot be checked automatically so easily, it requires manual checking. However, the Clang API is automatically called by our test
program which prints the output in a file so that the tester can check the result.
##
>>>> image ##
data/dowhile_clang.jpg;
Example of an input AST generated automatically of a do-while statement;
0.3
##
>>>> image ##
data/while_astNode.jpg;
Corresponding output (ASTNode tree data-structure), as the test program prints it;
0.3
##
>>>> paragraph ##
The manual checking first consists in comparing the shape of the input and output trees : it must be the same. As you can see it on the above examples,
this is a successful test in this regard. Then, the tester has to verify that all the nodes have been converted to the right kind of node (among ConcreteASTNode,
NullNode and OtherNode) and that the data has been correctly retrieved (line and column of the element in the source code, name of the element, etc).
##
>>>> paragraph ##
As the Clang AST treats the same way most kinds of code source element (most of the time, the format is \{ name codePosition args \}), there was no need of unitary tests
strictly speaking, we just ran our test on several examples, starting on small ones to detect minor mistakes and then more complicated ones in the hope that somewhere
in the mass of data we would see an unexpected edge-case.
##
>>> section ##
Testing the conversion to CFG
##
>>>> subsection ##
Partly automated test
##
>>>>> paragraph ##
The conversion consists in two steps :
##
>>>>> list ##
conversion to an intermediate tree data-structure : SourceCodeNode;
conversion from SourceCodeNode to the actual CFG
##
>>>>> paragraph ##
We decided not writing specific tests for the first step, as it is quite a simple one. Plus, it is uneasy to
validate the output automatically.  It is also important to say that
the way we printed the nodes after the whole transformation is representative of the behaviour of the first step of the conversion. Indeed, the content of the nodes
tells us if the first step went well, and the links in the graph inform us on the smooth functioning of the second step.
This way, it is fairly easy to discriminate a bug coming from the first step of conversion and a bug coming from the second step. Some bugs may
be harder to understand, nevertheless the structure of the code is exactly the same for both steps and is favorable to unitary testing.
As a consequence, to fix a complicated bug, the process is the following :
##
>>>>> list ##
identify the function of the second step that does not work well. This can be easily done thanks to our unitary tests, each one corresponding to one or a few functions.;
as this function takes the result of the first step as a parameter, print it (only in this function, to keep it readable !);
if the first step went well, investigate on the second step, otherwise investigate on the corresponding conversion method in the first step (same code structure).;
re-run the test. If the code passes the test, stop, otherwise go back to the first step.
##
>>>>> paragraph ##
The second step, contrarily to the first one, has been extensively tested. It requires manual check , however the graph nature of the CFG allowed us to use dot to generate nice graph images
corresponding to our computed CFG. It made it easier to check (visually and quickly) the output of the converter.
To sum up, here is what our test routine does :
##
>>>>> list ##
take a folder path as an input, it calls the Clang API on every cpp file in this folder;
parse each generated AST;
convert to SourceCodeNode (not unitary tested, effects will be visible in the final output);
convert to CFG (the actual implementation type being 	GraphNode[ProgramNode]);
print the CFG in dot format (text file);
call the dot command to generate an image representing the graph
##
>>>>> paragraph ##
Generating a graphic output has been a real time saver for us compared to debugging with a textual output as it enabled
us to test highly complex examples without getting confused by the mass of data.
##
>>>> subsection ##
Test cases
##
>>>>> paragraph ##
Contrarily to the parsing, the conversion to CFG implies very specific processing for different kinds of nodes,
which justifies the fact of writing unitary tests for each kind of node. These tests were ran for the first time during the sprint 1 and kept being executed
each time we made a change (generally motivated by reaching the compatibility constraints between the CFG and the model checker)
 to it during sprint 2 and 3. Here is how they prove the right functioning of the conversion :
##
>>>>> list ##
\textbf{Constraint 2.0.1} : can be checked visually (focusing on the links of the graph). Each statement of family of statements has its unitary test. Complicated tests prove the correctness of the interactions between different statements;
\textbf{Constraint 2.0.2} : met by default, as it does not impose anything;
\textbf{Constraint 2.0.3-4} : can be checked visually (focusing on content of the nodes of the graph).;
\textbf{Constraint 2.0.5} : the printing function traverses the graph in the order of apparition of the statements in the code, which proves the traversal possible.;
\textbf{Constraint 2.1.1} : this has been partially implemented. This can be checked visually as the function declaration nodes are completely expanded on the output graph. However, they are not linked to the function calls made in the main graph.
##
>>>>> paragraph ##
As an example, here are the input C++ code we used for testing the \textit{for} loop 
(the content is presented in one chunk, but it was actually in separate files to keep the graph clear) :
##
>>>>> code ##
java
// in all files 
int f() {
    return 3;
}
//////////   Example 1   //////////
int main(int argc, char** argv) {
    int j = 5 + f();
    if (j == 6);
    for (;;);
    int z = 22;
    if ((z-20)==19);
    f();
}
//////////   Example 2   //////////
int main(int argc, char** argv) {
    int j = 5 + f();
    for ( ; 5 < 3 ; );
    f();
}
//////////   Example 3   //////////
int main(int argc, char** argv) {
    f();
    for (int i=3 ; 5 < i ; i++)
        break;
    f();
}
//////////   Example 4   //////////
int main(int argc, char** argv) {
    int j = 5 + f();
    for (int i=3 ; 5 < i ; )
        i -= 5.0;
    f();
}
//////////   Example 5  //////////
int main(int argc, char** argv) {
    int j = f();
    for (int i=3 ; i<5 ; i++)
        i *= 2;
    f();
}
// and so on
##
>>>>> paragraph ##
As you can see, we try to cover all the possible cases for a \textit{for} loop : it takes the form \textit{for (a;b;c)} where a, b and c can be present or absent, which
makes us 8 cases to test. The nature of a, b, c (expression, declaration...) is not that important as there exists unitary tests for these kinds of nodes.
 We did exactly the same with the other elements (\textit{if, while, switch...}).
Then, we tried more complicated things to see if all continued to go well when different kinds of node were mixed. When we detected a bug,
we first identified which nodes were incorrectly rendered, fixed the problem by testing on the complicated example which revealed the issue, and finally
test the fix against the coorresponding unitary tests for non-regression.
##
>>>>> image ##
data/for_example;
On the top, the output obtained for the example 1, on the bottom, for the example 5;
0.40
##
>> chapter ##
Testing the model checking algorithm
##
>>> paragraph ##
Unlike the CFG part, there is a lot of room here for automated tests because there are many elementary operations that must return a specific result. We have written some generic methods 
that enable to automatically check the output rather than performing a manual check. This constitutes a very simple 
testing framework that we will present here. We will then present our process and results for the first iteration.
##
>>> section ##
Automated testing
##
>>>> paragraph ##
All the test functions rely on generic methods such as \textbf{assertEquals} or \textbf{assertTrue} that print
the result of the test with its number. This way, it is fairly easy to generate a log file and get the result of the tests
in the clearest way. You can find below an example of such test functions :
##
>>>> code ##
scala
def printMsg(failed: Boolean) = { 
        val msg = "\tTest %d %s".format(i,if (failed) "failed" else "passed") 
        if (failed) Console.err.println(msg)
        else        println(msg)
        i += 1
}
def assertEquals[T](t0: T, t1: T) = printMsg(t0 != t1)
def assertTrue(b: Boolean)       = printMsg(!b)
def compareEnv[T](envT1: Env[T], envT2: Env[T], expected: Env[T]) = assertEquals(envT1 interEnv envT2,expected)    
def testNeg[T](env: Env[T], envs: Env[T]*)  = assertEquals(!env,Set(envs: _*))
##
>>> section ##
Testing process
##
>>>> paragraph ##
For this part, we have made a lot of unitary tests. We first tested every operation defined on the Environment subclasses
because it constitutes the base of almost all computations performed by the algorithm. Once we were assured that these operations were correctly implemented,
we tested the operations of the ModelChecker, starting with the elementary operations to the compound operations. 
After each modification, we ran the whole test as it has a reasonable exection times (less than a second). These tests refer to the constraint 3.0.3. They were ran
during the sprints 1 and 2 until all the elementary operations passed their related tests.
##
>>>> image ##
data/test-log;
An example of a test log generated by our automatic tests;
0.85
##
>>>> paragraph ##
During the sprint 3, we wrote more advanced tests on small examples of graphs unrelated to any C++ code : just a few nodes and link to detect and fix obvious mistakes. This enabled
us to validate the constraint 3.0.1 and 3.0.4.
Finally, we tested the model checker against the CFG (constraint 3.0.2). Conveniently, the CFG conversion was already completely implemented when we starting to test the
model checker on C++ source codes. For this last testing step, we have had to manually check the output. Nevertheless, the test program was well-done and
easy-to-use. As the model checking algorithm takes some time to execute (a few seconds for a complicated property), it would have been unreasonable to execute the whole
tests, which processes many properties on different input codes (which have to be converted to CFG first). Therefore, we intensively used lazy values so that we can
test only the few properties we are interested in while having all the test cases defined in a single file.
The following snippet illustrates this strategy :
##
>>>> code ##
scala
// when executed, it calls the Clang API on hidden_var_def.cpp, parses the AST file, convert it into a CFG,
// and generate a dot graph to help visualizing
// checker1 will not be evaluated until a statement explicitely queries its value
lazy val checker1 = loadChecker("hidden_var_def")
lazy val test1 = {
	println("Testing the HIDDEN_VAR_DEF property...")
	printTest("Following lines are variable definitions that are hidden later in the code (may contain false positive results) :",
			checker1,HIDDEN_VAR_DEF)
}
	
lazy val checker2 = loadChecker("arith_pointer")
lazy val test2 = {
	println("Testing the ARITHMETIC_POINTER property...")
	printTest("Following lines contain an arithmetic expression involving a pointer :",checker2,ARITHMETIC_POINTER)
}

lazy val checker6 = loadChecker("unused_var")
lazy val test3 = {
       println("Testing the UNUSED_DECALRED_VAR property...")
       printTest("Following lines contain variable definition that are not used :",checker3,UNUSED_DECALRED_VAR)
}
// only test 1 is executed
test1
##
>>>> paragraph ##
The output is printed in the following form :
##
>>>> image ##
data/test-log2;
Log file for the model checker testing program;
0.8
##
>>>> paragraph ##
The properties we used for our tests were inspired from the MISRAC++ (a set of rules of good and bad practices) :
##
>>>> list ##
<< There should not be function calls returning a value which is never used nor stored >>;
<< There should not be hidden declarations in the code >>;
<< There should not be arithmetic operations on pointer types other than the unary increment and decrement >>;
<< There should not be declarations that are never used in any computation of any execution path >>;
...
##
>>>> paragraph ##
To each property corresponds a C++ input file. Below are the files corresponding to the two first properties.
##
>>>> code ##
java
/**
 * UNUSED_FUNCTION_VALUE
 */
int f(int a, int b){
	return 1;
}

void g() { }

void h(int a) {
	a = 0;
}

float fun() {
	return 1.0f;
}

int main(int argc, char** argv) {
	g();
	int i = f(5,6) + 3;
 	// should return this line
	f(3,4);
	if (fun()) i = 0;
	// should return this line
	else       fun();
	while (f(5,fun())) {
		// should return this line
		f(1,6) + 1;
		// should return this line, with the right column (pointing at 5*(i - 9*fun()))
		if (f(4,2) + fun()) 5*(i - 9*fun());
	}
	return 0;
}

/**
 * HIDDEN_VAR_DEF
 */
int f(int q) {
    return q+2;
}

int main(int argc, char** argv) {
	int j = 0, q = 2;

	{
		// should return this line
		int j = 3;
		{
			// should not return this line (we are checking declarations of the same type)
			float j = 15.0f;
			{
				// should return this line
				float j;
				{
					// should not return this line (but it will, as a false positive result)
					int j = 18;
				}
			}
		}
	}
    if (j == 2)
    	f(q);

    if (false){
    	q = 7;
    }
    else {
    	while (true) {
    		int k = 0;
    	}
    }

    // should not return this line (but it will, as a false positive result)
    int k;
    return 0;
}
##
